{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a02078-96b3-4f14-9669-fb49f4edb4c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import TensorDataset,DataLoader, Dataset\n",
    "import argparse\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c014230b-cda3-4cbc-920a-2d5b16570858",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from net_dev import GNNStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f5d7406-8f67-45bd-add7-cb4a8fe7a97b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7e2de2d-8c0d-4ab8-8090-5e5621f13034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'arch': 'dyGIN2d', \n",
    "    'dataset': 'extval_MIMIC_eICU',\n",
    "    'num_layers': 2,  # the number of GNN layers  3\n",
    "    'groups': 360,  # the number of time series groups (num_graphs)\n",
    "    'pool_ratio': 0.0,  # the ratio of pooling for nodes # initially 0.1 but changed to 0 because the node number was decreasing\n",
    "    'kern_size': [3,3],  # list of time conv kernel size for each layer [9,5,3]\n",
    "    'in_dim': 64,  # input dimensions of GNN stacks\n",
    "    'hidden_dim': 64,  # hidden dimensions of GNN stacks\n",
    "    'out_dim': 64,  # output dimensions of GNN stacks\n",
    "    'workers': 0,  # number of data loading workers\n",
    "    'epochs': 80,  # number of total epochs to run\n",
    "    'batch_size': 8,  # mini-batch size, this is the total batch size of all GPUs\n",
    "    'val_batch_size': 8,  # validation batch size\n",
    "    'lr': 0.0001,  # initial learning rate\n",
    "    'weight_decay': 1e-4,  # weight decay\n",
    "    'evaluate': False,  # evaluate model on validation set\n",
    "    'seed': 4,  # seed for initializing training\n",
    "    'gpu': 0,  # GPU id to use\n",
    "    'use_benchmark': True,  # use benchmark\n",
    "    'tag': 'date',  # the tag for identifying the log and model files\n",
    "    'loss':'bce',\n",
    "    'resample':'1hour',\n",
    "    'series_length':720,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4c9a44f-2214-47c8-8de8-d47d7ba5202d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "              \n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "        \n",
    "        # print(output, target)\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            try:\n",
    "                res.append(correct_k.mul_(100.0 / batch_size))\n",
    "            except:\n",
    "                res.append(0)\n",
    "\n",
    "        return res\n",
    "\n",
    "def log_msg(message, log_file):\n",
    "    with open(log_file, 'a') as f:\n",
    "        print(message, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78cfcef6-423b-4003-9b84-70e4a07bd9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, data_path, labels_path, mask_path):\n",
    "        self.data = np.load(data_path)\n",
    "        self.labels = np.load(labels_path)\n",
    "        self.mask = np.load(mask_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        labels = self.labels[idx]\n",
    "        mask = self.mask[idx]\n",
    "        return torch.tensor(data, dtype=torch.float32), torch.tensor(labels, dtype=torch.int64), torch.tensor(mask, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985f6ad2-2755-4502-b275-ea9e9b8f9539",
   "metadata": {},
   "source": [
    "## load training data mimic and testing data eicu below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00047e0c-ad38-424a-81ff-b5b7637f30fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_default_train_val_test_loader():\n",
    "    # Generic absolute paths (replace with actual paths when deploying)\n",
    "    data_train_path  = '/path/to/train_data_mimic_extval.npy'\n",
    "    data_valid_path  = '/path/to/valid_data_eicu_extval.npy'\n",
    "    data_test_path   = '/path/to/test_data_eicu_extval.npy'\n",
    "\n",
    "    label_train_path = '/path/to/train_label_mimic_extval.npy'\n",
    "    label_valid_path = '/path/to/valid_label_eicu_extval.npy'\n",
    "    label_test_path  = '/path/to/test_label_eicu_extval.npy'\n",
    "\n",
    "    mask_train_path  = '/path/to/train_mask_mimic_extval.npy'\n",
    "    mask_valid_path  = '/path/to/valid_mask_eicu_extval.npy'\n",
    "    mask_test_path   = '/path/to/test_mask_eicu_extval.npy'\n",
    "\n",
    "    # Create datasets using NumpyDataset class\n",
    "    train_dataset = NumpyDataset(data_train_path, label_train_path, mask_train_path)\n",
    "    val_dataset   = NumpyDataset(data_valid_path, label_valid_path, mask_valid_path)\n",
    "    test_dataset  = NumpyDataset(data_test_path, label_test_path, mask_test_path)\n",
    "\n",
    "    # Constants\n",
    "    num_nodes = 63\n",
    "    seq_length = 720\n",
    "    num_classes = 2\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True,  num_workers=1, pin_memory=True, prefetch_factor=2)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=4, shuffle=False, num_workers=1, pin_memory=True, prefetch_factor=2)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=4, shuffle=False, num_workers=1, pin_memory=True, prefetch_factor=2)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, num_nodes, seq_length, num_classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "991c1bc0-5e2d-4ea2-9691-3e2fac259bd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main_work(args):\n",
    "    \n",
    "    random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    " \n",
    "    \n",
    "    # init metrics\n",
    "    best_acc1 = 0\n",
    "    best_roc  = 0\n",
    "    best_pr   = 0\n",
    "    best_f1   = 0\n",
    "    best_mcc  = 0\n",
    "    \n",
    "    best_test_acc1 = 0\n",
    "    best_test_roc  = 0\n",
    "    best_test_pr   = 0\n",
    "    best_test_f1   = 0\n",
    "    best_test_mcc  = 0    \n",
    "    \n",
    "    if args['tag'] == 'date':\n",
    "        local_date = time.strftime('%m.%d %H:%M', time.localtime(time.time()))\n",
    "        args['tag'] = local_date\n",
    "        \n",
    "    # Use the 'tag' which now contains either the date or a custom tag along with the dataset name for the directory\n",
    "    run_dir_name = f\"{args['dataset']}_{args['tag']}\"\n",
    "   \n",
    "    # Base directory for saving models\n",
    "    base_model_save_dir = \"saved_models\"\n",
    "   \n",
    "    # Specific directory for this run\n",
    "    specific_model_save_dir = os.path.join(base_model_save_dir, run_dir_name)\n",
    "    os.makedirs(specific_model_save_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Models will be saved in: {specific_model_save_dir}\")        \n",
    "\n",
    "\n",
    "    log_file = 'log/{}_gpu{}_{}_{}_exp.txt'.format(args['tag'], args['gpu'], args['arch'], args['dataset'])\n",
    "    \n",
    "    \n",
    "    if args['gpu'] is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args['gpu']))\n",
    "\n",
    "\n",
    "    # dataset\n",
    "    train_loader, val_loader, test_loader, num_nodes, seq_length, num_classes = get_default_train_val_test_loader(args)\n",
    "    \n",
    "    print('features / nodes', num_nodes,'total time graphs',args['groups'],'time series length',seq_length,'classes', num_classes)\n",
    "    \n",
    "    # training model from net.py\n",
    "    model = GNNStack(gnn_model_type=args['arch'], num_layers=args['num_layers'], \n",
    "                     groups=args['groups'], pool_ratio=args['pool_ratio'], kern_size=args['kern_size'], \n",
    "                     in_dim=args['in_dim'], hidden_dim=args['hidden_dim'], out_dim=args['out_dim'], \n",
    "                     seq_len=seq_length, num_nodes=num_nodes, num_classes=num_classes)\n",
    "    \n",
    "    print(f\"Number of parameters in TGN: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "    # print & log\n",
    "    log_msg('epochs {}, lr {}, weight_decay {}'.format(args['epochs'], args['lr'], args['weight_decay']), log_file)\n",
    "    \n",
    "    log_msg(str(args), log_file)\n",
    "\n",
    "\n",
    "    # determine whether GPU or not\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"Warning! Using CPU!!!\")\n",
    "    elif args['gpu'] is not None:\n",
    "        torch.cuda.set_device(args['gpu'])\n",
    "\n",
    "        # collect cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        model = model.cuda(args['gpu'])\n",
    "        if args['use_benchmark']:\n",
    "            cudnn.benchmark = True\n",
    "        print('Using cudnn.benchmark.')\n",
    "    else:\n",
    "        print(\"Error! We only have one gpu!!!\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss().cuda(args['gpu'])\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    \n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "    # validation\n",
    "    if args['evaluate']:\n",
    "        validate(val_loader, model, criterion, args)\n",
    "        return\n",
    "\n",
    "    # train & valid\n",
    "    print('****************************************************')\n",
    "    print('Dataset: ', args['dataset'])\n",
    "\n",
    "    dataset_time = AverageMeter('Time', ':6.3f')\n",
    "\n",
    "    loss_train = []\n",
    "    acc_train = []\n",
    "    loss_val = []\n",
    "    acc_val = []\n",
    "    epoches = []\n",
    "    \n",
    "    ########################\n",
    "    loss_test = []\n",
    "    acc_test  = []\n",
    "    #######################\n",
    "    \n",
    "    ###### more lists to have values\n",
    "    roc_train = []\n",
    "    pr_train  = []\n",
    "    f1_train  = []\n",
    "    mcc_train = []\n",
    "    \n",
    "    roc_val   = []\n",
    "    pr_val    = []\n",
    "    f1_val    = []\n",
    "    mcc_val   = []\n",
    "    ##################################################\n",
    "    roc_test   = []\n",
    "    pr_test    = []\n",
    "    f1_test    = []\n",
    "    mcc_test   = []     \n",
    "    #################################################\n",
    "    end = time.time()\n",
    "    for epoch in tqdm(range(args['epochs'])):\n",
    "    # for epoch in range(args['epochs']):\n",
    "        epoches += [epoch]\n",
    "\n",
    "        # train for one epoch\n",
    "        acc_train_per, loss_train_per, output_train_per, target_train_per = train(train_loader, model, criterion, optimizer, lr_scheduler, args)\n",
    "        \n",
    "        acc_train += [acc_train_per]\n",
    "        loss_train += [loss_train_per]\n",
    "        # calculate metric\n",
    "        # print(len(target_train_per),len(output_train_per))\n",
    "        auc_roc_value_train = roc_auc_score(target_train_per, output_train_per)\n",
    "        auc_pr_value_train = average_precision_score(target_train_per, output_train_per)\n",
    "        p2l_value_train = np.where(np.array(output_train_per) >= 0.5, 1, 0)\n",
    "        f1_value_train = f1_score(target_train_per, p2l_value_train.tolist())\n",
    "        mcc_value_train= matthews_corrcoef(target_train_per,p2l_value_train.tolist())\n",
    "        \n",
    "        #new code\n",
    "        roc_train += [auc_roc_value_train]\n",
    "        pr_train  += [auc_pr_value_train]\n",
    "        f1_train  += [f1_value_train]\n",
    "        mcc_train  += [mcc_value_train]\n",
    "\n",
    "        msg = f'TRAIN, epoch {epoch}, train_loss {loss_train_per}, train_acc {acc_train_per}, train_roc {auc_roc_value_train:.5f}, \\\n",
    "                train_pr {auc_pr_value_train:.5f}, train_f1 {f1_value_train:.5f},train_mcc {mcc_value_train:.5f}'\n",
    "\n",
    "        print(f'TRAIN, epoch {epoch}, train_loss {loss_train_per:.5f}, train_roc {auc_roc_value_train:.5f}, train_pr {auc_pr_value_train:.5f}, train_f1 {f1_value_train:.5f}, train_mcc {mcc_value_train:.5f}')\n",
    "        # tqdm.write(f'TRAIN, epoch {epoch}, train_loss {loss_train_per:.5f}, train_roc {auc_roc_value_train:.5f}, train_pr {auc_pr_value_train:.5f}, train_f1 {f1_value_train:.5f}, train_mcc {mcc_value_train:.5f}')\n",
    "        log_msg(msg, log_file)\n",
    "\n",
    "        \n",
    "        # evaluate on validation set\n",
    "        acc_val_per, loss_val_per, output_val_per, target_val_per = validate(val_loader, model, criterion, args)\n",
    "\n",
    "        acc_val  += [acc_val_per]\n",
    "        loss_val += [loss_val_per]\n",
    "        #calculate metric\n",
    "        # calculate metric\n",
    "        # print(len(target_val_per),len(output_val_per))\n",
    "        auc_roc_value_val = roc_auc_score(target_val_per, output_val_per)\n",
    "        auc_pr_value_val = average_precision_score(target_val_per, output_val_per)\n",
    "        p2l_value_val = np.where(np.array(output_val_per) >= 0.5, 1, 0)\n",
    "        f1_value_val = f1_score(target_val_per, p2l_value_val.tolist())\n",
    "        mcc_value_val= matthews_corrcoef(target_val_per,p2l_value_val.tolist())\n",
    "        #new code\n",
    "\n",
    "        msg = f'VAL, epoch {epoch}, val_loss {loss_val_per}, val_acc {acc_val_per}, val_roc {auc_roc_value_val:.5f}, val_pr {auc_pr_value_val:.5f},val_f1 {f1_value_val:.5f}, val_mcc {mcc_value_val:.5f}'\n",
    "        \n",
    "        print(f'VAL, epoch {epoch}, val_loss {loss_val_per:.5f}, val_roc {auc_roc_value_val:.5f}, val_pr {auc_pr_value_val:.5f},val_f1 {f1_value_val:.5f}, val_mcc {mcc_value_val:.5f}')\n",
    "        # tqdm.write(f'VAL, epoch {epoch}, val_loss {loss_val_per:.5f}, val_roc {auc_roc_value_val:.5f}, val_pr {auc_pr_value_val:.5f},val_f1 {f1_value_val:.5f}, val_mcc {mcc_value_val:.5f}')\n",
    "        log_msg(msg, log_file)\n",
    "        #########################################################################################################################\n",
    "        # evaluate on test set\n",
    "        acc_test_per, loss_test_per, output_test_per, target_test_per = validate(test_loader, model, criterion, args)\n",
    "\n",
    "        acc_test   += [acc_test_per]\n",
    "        loss_test  += [loss_test_per]\n",
    "        #calculate metric\n",
    "        # calculate metric\n",
    "        # print(len(target_val_per),len(output_val_per))\n",
    "        auc_roc_value_test = roc_auc_score(target_test_per, output_test_per)\n",
    "        auc_pr_value_test  = average_precision_score(target_test_per, output_test_per)\n",
    "        p2l_value_test = np.where(np.array(output_test_per) >= 0.5, 1, 0)\n",
    "        f1_value_test = f1_score(target_test_per, p2l_value_test.tolist())\n",
    "        mcc_value_test= matthews_corrcoef(target_test_per,p2l_value_test.tolist())\n",
    "        #new code\n",
    "\n",
    "        msg = f'TEST, epoch {epoch},test_loss {loss_test_per},test_acc {acc_test_per},test_roc {auc_roc_value_test:.5f},test_pr {auc_pr_value_test:.5f},test_f1 {f1_value_test:.5f}, test_mcc {mcc_value_test:.5f}'\n",
    "        print(f'TEST, epoch {epoch}, test_loss {loss_test_per:.5f}, test_roc {auc_roc_value_test:.5f}, test_pr {auc_pr_value_test:.5f}, test_f1 {f1_value_test:.5f}, test_mcc {mcc_value_test:.5f}')\n",
    "        # tqdm.write(f'TEST, epoch {epoch}, test_loss {loss_test_per:.5f}, test_roc {auc_roc_value_test:.5f}, test_pr {auc_pr_value_test:.5f}, test_f1 {f1_value_test:.5f}, test_mcc {mcc_value_test:.5f}')\n",
    "        log_msg(msg, log_file)        \n",
    "        \n",
    "        #########################################################################################################################\n",
    "        # remember best acc\n",
    "        best_acc1 = max(acc_val_per, best_acc1)\n",
    "        best_roc  = max(auc_roc_value_val, best_roc)\n",
    "        best_pr   = max(auc_pr_value_val, best_pr)\n",
    "        best_f1   = max(f1_value_val, best_f1)\n",
    "        best_mcc   = max(mcc_value_val, best_mcc)\n",
    "        \n",
    "        #########################################################################################################################\n",
    "        \n",
    "        best_test_acc1 = max(acc_test_per, best_test_acc1)\n",
    "        best_test_roc  = max(auc_roc_value_test, best_test_roc)\n",
    "        best_test_pr   = max(auc_pr_value_test, best_test_pr)\n",
    "        best_test_f1   = max(f1_value_test, best_test_f1)\n",
    "        best_test_mcc   = max(mcc_value_test, best_test_mcc)\n",
    " \n",
    "\n",
    "    #     wandb.log({\"train_loss\": loss_train_per, \"train_roc\": auc_roc_value_train, \"train_pr\": auc_pr_value_train, \\\n",
    "    #                \"val_loss\": loss_val_per, \"val_roc\": auc_roc_value_val, \"val_pr\": auc_pr_value_val, \"best_val_roc\": best_roc, \"best_val_pr\": best_pr})\n",
    "    # wandb.finish()\n",
    "    \n",
    "    \n",
    "    \n",
    "        # Construct the filename with metrics\n",
    "        auc_roc_value_val_scalar = auc_roc_value_val.item() if isinstance(auc_roc_value_val, torch.Tensor) else auc_roc_value_val\n",
    "        auc_pr_value_val_scalar  = auc_pr_value_val.item() if isinstance(auc_pr_value_val, torch.Tensor) else auc_pr_value_val\n",
    "        f1_value_val_scalar      = f1_value_val.item() if isinstance(f1_value_val, torch.Tensor) else f1_value_val\n",
    "        mcc_value_val_scalar     = mcc_value_val.item() if isinstance(mcc_value_val, torch.Tensor) else mcc_value_val\n",
    "        #########################################################################################################################\n",
    "        auc_roc_value_test_scalar = auc_roc_value_test.item() if isinstance(auc_roc_value_test, torch.Tensor) else auc_roc_value_test\n",
    "        auc_pr_value_test_scalar  = auc_pr_value_test.item() if isinstance(auc_pr_value_test, torch.Tensor) else auc_pr_value_test\n",
    "        f1_value_test_scalar      = f1_value_test.item() if isinstance(f1_value_test, torch.Tensor) else f1_value_test\n",
    "        mcc_value_test_scalar      = mcc_value_test.item() if isinstance(mcc_value_test, torch.Tensor) else mcc_value_test        \n",
    "        #########################################################################################################################\n",
    "        # Now use these scalar values in the filename\n",
    "        filename = f\"model_epoch_{epoch}.pth\"\n",
    "       \n",
    "        # Continue with the existing logic to save the model\n",
    "        model_path = os.path.join(specific_model_save_dir, filename)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # measure elapsed time\n",
    "    dataset_time.update(time.time() - end)\n",
    "\n",
    "    # log & print the best_acc\n",
    "    msg = f'\\n\\n * BEST_ACC: {best_acc1}\\n * TIME: {dataset_time}\\n'\n",
    "    log_msg(msg, log_file)\n",
    "\n",
    "    print(f' * best_acc1: {best_acc1}, best_roc: {best_roc}, best_pr: {best_pr}, best_f1: {best_f1}, best_mcc: {best_mcc}')\n",
    "    print(f' * best_test_acc1: {best_test_acc1}, best_test_roc: {best_test_roc}, best_test_pr: {best_test_pr}, best_test_f1: {best_test_f1}, best_test_mcc: {best_test_mcc}')\n",
    "    print(f' * time: {dataset_time}')\n",
    "    print('****************************************************')\n",
    "\n",
    "\n",
    "    # collect cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, lr_scheduler, args):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc', ':6.2f')\n",
    "    # met_roc = AverageMeter('ROC', ':6.2f')\n",
    "    # met_pr = AverageMeter('PR', ':6.2f')\n",
    "    \n",
    "    output_list = []\n",
    "    target_list = [] \n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for count, (data, label, mask) in enumerate(train_loader):\n",
    "\n",
    "        # data in cuda\n",
    "        \n",
    "        ### data filter\n",
    "        \n",
    "        # data_fil = torch.index_select(data, 2, indexes_of_1_torch) ## this selects the relevant features\n",
    "        label = torch.where(label == -1, torch.tensor(0), label)\n",
    "        # data = data_fil.cuda(args['gpu']).type(torch.float)\n",
    "        data = data.cuda(args['gpu']).type(torch.float)\n",
    "        data = data.permute(0, 2, 1).unsqueeze(1)           \n",
    "        \n",
    "        mask = mask.cuda(args['gpu']).type(torch.bool)\n",
    "        label = label.cuda(args['gpu']).type(torch.long)\n",
    "\n",
    "        # compute output\n",
    "        output = model(data)\n",
    "        # print(len(output))\n",
    "        # print('output', output.shape, 'mask', mask.shape)\n",
    "        out_flat = torch.masked_select(output, mask.unsqueeze(-1)).reshape(-1, output.shape[-1])\n",
    "        # print(output)\n",
    "        # print(output.shape, mask.shape, out_flat.shape)\n",
    "        # break\n",
    "        # out_flat = torch.masked_select(output[:,:,1], mask)\n",
    "        \n",
    "\n",
    "        label_flat = torch.masked_select(label, mask)\n",
    "        # print('output',output.shape, 'mask', mask.shape,'out_flat', out_flat.shape, 'label', label.shape, 'label_flat', label_flat.shape)\n",
    "        loss = criterion(out_flat, label_flat)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        acc1 = accuracy(out_flat, label_flat, topk=(1, 1))\n",
    "        \n",
    "        output_np = torch.softmax(out_flat, dim=1).detach().cpu().numpy()[:,1].tolist()\n",
    "        \n",
    "        target_np = label_flat.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        # print(output_np, target_np)\n",
    "        \n",
    "        losses.update(loss.item(), data.size(0))\n",
    "        top1.update(acc1[0], data.size(0))\n",
    "        \n",
    "        # met_roc.update(roc, data.size(0))\n",
    "        # met_pr.update(pr, data.size(0))\n",
    "        output_list += output_np\n",
    "        target_list += target_np\n",
    "\n",
    "        # compute gradient and do Adam step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    lr_scheduler.step(top1.avg)\n",
    "\n",
    "    return top1.avg, losses.avg, output_list, target_list\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, args):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    # met_roc = AverageMeter('ROC', ':6.2f')\n",
    "    # met_pr = AverageMeter('PR', ':6.2f')\n",
    "    output_list = []\n",
    "    target_list = [] \n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for count, (data, label, mask) in enumerate(val_loader):\n",
    "\n",
    "            label = torch.where(label == -1, torch.tensor(0), label)\n",
    "            \n",
    "            if args['gpu'] is not None:\n",
    "                \n",
    "                # data = data_fil.cuda(args['gpu'], non_blocking=True).type(torch.float)\n",
    "                data = data.cuda(args['gpu'], non_blocking=True).type(torch.float)\n",
    "                data = data.permute(0, 2, 1).unsqueeze(1)      \n",
    "            if torch.cuda.is_available():\n",
    "                label = label.cuda(args['gpu'], non_blocking=True).type(torch.long)\n",
    "                mask  = mask.cuda(args['gpu'], non_blocking=True).type(torch.bool)\n",
    "\n",
    "            # compute output\n",
    "            output = model(data)\n",
    "            \n",
    "            out_flat = torch.masked_select(output, mask.unsqueeze(-1)).reshape(-1, output.shape[-1])\n",
    "            label_flat = torch.masked_select(label, mask)\n",
    "\n",
    "            loss = criterion(out_flat, label_flat)\n",
    "            \n",
    "            output_np = torch.softmax(out_flat, dim=1).detach().cpu().numpy()[:,1].tolist()\n",
    "            target_np = label_flat.detach().cpu().numpy().tolist()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1 = accuracy(out_flat, label_flat, topk=(1, 1))\n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            top1.update(acc1[0], data.size(0))\n",
    "            \n",
    "            output_list += output_np\n",
    "            target_list += target_np\n",
    "            \n",
    "            # met_roc.update(roc, data.size(0))\n",
    "            # met_pr.update(pr, data.size(0))\n",
    "\n",
    "    return top1.avg, losses.avg, output_list, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb92317-8e78-4770-99b5-32bf6aba679d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model(model_path, model_class, *model_args, **model_kwargs):\n",
    "    \"\"\"\n",
    "    Load the model from a saved state dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - model_path: Path to the saved model state dictionary.\n",
    "    - model_class: The class of the model to instantiate.\n",
    "    - model_args: Positional arguments for the model class instantiation.\n",
    "    - model_kwargs: Keyword arguments for the model class instantiation.\n",
    "\n",
    "    Returns:\n",
    "    - model: The loaded model ready for prediction.\n",
    "    \"\"\"\n",
    "    # Instantiate the model\n",
    "    model = model_class(*model_args, **model_kwargs)\n",
    "    # Load the saved state dictionary\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "def predict(test_loader, model, criterion, args):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    # met_roc = AverageMeter('ROC', ':6.2f')\n",
    "    # met_pr = AverageMeter('PR', ':6.2f')\n",
    "    output_list = []\n",
    "    target_list = [] \n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for count, (data, label, mask) in enumerate(test_loader):\n",
    "            \n",
    "           \n",
    "            label = torch.where(label == -1, torch.tensor(0), label)\n",
    "            \n",
    "            if args['gpu'] is not None:\n",
    "                \n",
    "                # data = data_fil.cuda(args['gpu'], non_blocking=True).type(torch.float)\n",
    "                data = data.cuda(args['gpu'], non_blocking=True).type(torch.float)\n",
    "                data = data.permute(0, 2, 1).unsqueeze(1)      \n",
    "            if torch.cuda.is_available():\n",
    "                label = label.cuda(args['gpu'], non_blocking=True).type(torch.long)\n",
    "                mask  = mask.cuda(args['gpu'], non_blocking=True).type(torch.bool)\n",
    "\n",
    "            # compute output\n",
    "            output, g_const_adj, adj, adj_notopk = model(data)\n",
    "                      \n",
    "            out_flat = torch.masked_select(output, mask.unsqueeze(-1)).reshape(-1, output.shape[-1])\n",
    "            label_flat = torch.masked_select(label, mask)\n",
    "\n",
    "            loss = criterion(out_flat, label_flat)\n",
    "            \n",
    "            output_np = torch.softmax(out_flat, dim=1).detach().cpu().numpy()[:,1].tolist()\n",
    "            target_np = label_flat.detach().cpu().numpy().tolist()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            acc1 = accuracy(out_flat, label_flat, topk=(1, 1))\n",
    "            losses.update(loss.item(), data.size(0))\n",
    "            top1.update(acc1[0], data.size(0))\n",
    "            \n",
    "            output_list += output_np\n",
    "            target_list += target_np\n",
    "            \n",
    "            # met_roc.update(roc, data.size(0))\n",
    "            # met_pr.update(pr, data.size(0))\n",
    "\n",
    "    return top1.avg, losses.avg, output_list, target_list, g_const_adj, adj, adj_notopk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "adbbabaa-9dd4-49ae-be54-909c542b649e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "main_work(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
